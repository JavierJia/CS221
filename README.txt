1. About Crawler
We skipped all the url with "[?=]+' 
While crawling, we found that it trap in ftp.ics.uci.edu and fano.ics.uci.edu 
so we restarted by skip the afterward url start by those two site

2. About Data
We store the text into one single file. we keep it url/title/text by 3 lines each page
Then we run the 'count.py' to caculate the statistical result.

3. About Extra Credit

