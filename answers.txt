1. How much time did it take to crawl the entire domain?
20h

2. How many unique pages did you find in the entire domain? (Uniqueness is established by the URL)
raw url:
152339

remove-dupblicate by bigram signature:
# python cluster.py -d < data/web.data > data/web.signature
68123

cluster:
# python cluster.py -c < data/web.signature > data/web.sketch
we got 216 pages have overlaped shingles of 8/10 
then after union_find
# python union_find < data/web.sketch
we got 26 class
Then the total pages, after cluster is 

68123 - 216 + 26 = 67933

3. How many subdomains did you find? Submit the list of subdomains ordered alphabetically and the number of unique pages detected in each subdomain. The file should be called Subdomains.txt, and its content should be lines containing URL, number http://vision.ics.uci.edu, 10 ( not the actual number here) etc.
72

4. What is the longest page in terms of number of words? (HTML markup doesn’t count as words)
http://www.ics.uci.edu/~xhx/project/MotifMap/SNP/motif_sites_overlap_db_snp.list.html
12323433 words

5. What are the 500 most common words in this domain? (Ignore English stop words, which can be found, for example, here) Submit the list of common words ordered by frequency. The file should be called CommonWords.txt.
we omit the common word and also single letters
using cmd:
tail -n+3 count.txt | head -500 > CommonWords.txt

6. What are the 20 most common 2-grams? (again ignore English stop words) A 2-gram, in this case, is a sequence of 2 words that aren’t stop words and that haven’t had a stop word in between them. Submit the list of 20 2-grams ordered by frequency. The file should be called Common2Grams.txt.
see Common2Grams.txt
